# 动作行为分析-第一小组实验报告
## 代码分析
### 数据集v1
```python
class SSV1Dataset(Dataset):
    def __init__(self, root, mode='train', transform=None):
        super(SSV1Dataset, self).__init__()
        self.transform = transform
        self.root = root
        self.data_dict = np.loadtxt(os.path.join(root, mode +'_videofolder.txt'), dtype=np.long)

    def pil_loader(self, path):
        with open(path, 'rb') as f:
            img = Image.open(f)
            return self.transform(img.convert('RGB')).numpy()

    def loader(self, videoitem):
        path = os.path.join(self.root, '20bn-something-something-v1', str(videoitem[0]))
        index = np.linspace(0, videoitem[1], frame_count + 2, dtype=np.long)
        images = []
        for i in index[1:-1]:
            images.append(self.pil_loader(path + '/{:05d}.jpg'.format(i + 1)))
        return images
    
    def __getitem__(self, index):
        videoitem = self.data_dict[index]
        sample = self.loader(videoitem)
        return torch.Tensor(sample), videoitem[2]
    
    def __len__(self):
        return len(self.data_dict)
```
### 数据集v2
由于数据集v2过大，使用ffmpeg抽帧可能需要占据1T左右的磁盘，所以没有抽帧，采用opencv抽帧
```python
class SSV2Dataset(Dataset):
    def __init__(self, root, mode='train', transform=None):
        super(SSV2Dataset, self).__init__()
        self.transform = transform
        
        data_jsonfile = os.path.join(root, 'something-something-v2-' + mode +'.json')
        if not os.path.exists(data_jsonfile):
            print('{} is not exist.'.format(data_jsonfile))

        with open(data_jsonfile, 'r', encoding='utf-8') as f:
            self.data_dict = json.load(f)
            if mode != 'test':
                label_jsonfile = os.path.join(root, 'something-something-v2-labels.json')
                if not os.path.exists(label_jsonfile):
                    print('{} is not exist.'.format(label_jsonfile))
                with open(label_jsonfile, 'r', encoding='utf-8') as f:
                    labels = json.load(f)
            for item in self.data_dict:
                item['id'] = os.path.join(root, '20bn-something-something-v2', item['id'] + '.webm')
                if mode != 'test':
                    item['classidx'] = int(labels[item['template'].replace('[', '').replace(']', '')])

    def loader(self, path):
        vc = cv2.VideoCapture(path)
        '''
        这里本来使用的是获取视频总帧数，然后等间距(四舍五入)抽取帧数
        但是在实验过程中, 有一部分视频在抽取完一定帧数后,抽取不到后面的帧数
        eg: 63163.webm 该视频有51帧，但是只能抽取到第38帧，后面的帧数均抽取失败
        '''
        # total = vc.get(cv2.CAP_PROP_FRAME_COUNT)
        # index = np.linspace(0, total, frame_count + 1, dtype=np.long)
        # count = 0
        images = []
        if vc.isOpened(): #判断是否正常打开
            rval , frame = vc.read()
        else:
            rval = False
        while rval:
            images.append(self.transform(Image.fromarray(frame)).numpy())
            # count += 1
            # vc.set(cv2.CAP_PROP_POS_FRAMES, index[count])
            rval , frame = vc.read()
        # if len(images) != 10:
            # print(path, total)
        index = np.linspace(0, len(images), frame_count + 2, dtype=np.long)
        vc.release()
        return np.array(images)[index[1:-1]]
    
    def __getitem__(self, index):
        path, target = self.data_dict[index]['id'], self.data_dict[index]['classidx']
        sample = self.loader(path)
        return torch.Tensor(sample), target
    
    def __len__(self):
        return len(self.data_dict)
```
### 模型
这里只提取关键模型, 其中resnet50模型部分使用的是官方代码，做了一些改动使得代码能够输出1024个特征，并且为了适应分辨率不一的情况，将resnet的AvgPool改为了AdaptiveAvgPool。

```python
class BasicModel(nn.Module):
    def __init__(self, embedding_dim, hidden_dim, num_classes=1000):
        super(BasicModel, self).__init__()
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.videoframe = frame_count
        self.resnet50 = resnet50(embedding_dim)
        '''
        在这里使用多卡训练而不是对整个模型使用是因为cnn网络转换到rnn网络中间需要view和transpose操作,
        大部分情况下都会导致view的结果和需要的结果大相径庭。
        '''
        if torch.cuda.device_count() > 1:
            self.resnet50 = nn.DataParallel(self.resnet50,device_ids=[4,5,6,7])

        self.lstm = nn.LSTM(embedding_dim, hidden_dim)
        self.avgpool = nn.AdaptiveAvgPool2d((1, hidden_dim))
        self.fc = nn.Linear(hidden_dim, num_classes)
        self.activate = nn.Sigmoid()

    def forward(self, x):
        '''
        x 是输入 (batch_size * frames) * channel * w * h

        embeds (batch_size * frames) * 1024, 因为lstm和时序有关，所以先将 embeds 化为 batch_size * frames * 1024
        然后转置为 frames * batch_size * 1024, 因为torch的lstm需要时序信息在第一维度

        lstm_out frames * batch_size * 4096, 转置为 batch_size * frames * 4096
        取多帧平均值
            * 使用AdaptiveAvgPool2d是为了模型能够使用 3 帧预训练模型, 后面换成 6 帧不需要从头开始训练
            * 取平均值而不直接使用最后一帧的信息, 只要是考虑到不知道直接取lstm_out = lstm_out[-1]会不会导致反馈调节失败
            * 没使用加权平均是卷积核大小不好确定, 权值的个数需要随着帧数的改变而改变
        '''
        embeds = self.resnet50(x)
        lstm_out, _ = self.lstm(embeds.view(-1, self.videoframe, self.embedding_dim).transpose(1, 0))
        lstm_avg_out = self.avgpool(lstm_out.transpose(1, 0).view(-1, 1, self.videoframe, self.hidden_dim))
        tag_scores = self.fc(lstm_avg_out.view(-1, self.hidden_dim))
        return tag_scores
```
## 实验数据
因为在v1上训练的时间少，从开始的1~3小时一轮，所以实验结果不是很多，v2完整数据所需时间更长，按预计应该是7小时左右一轮，所以没有完整训练集的数据(之前的实验数据因为代码中存在问题不计)
| 数据集 | 帧数 | 学习率 | 衰减系数 | 优化方法 | 训练集正确率 | 测试集正确率 | 备注 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| v1 | 3 | 1e-3 | 0.83 (1/1.2) | Adam | 28% | 4% | 训练到28%后不再上升, 停止, 改用6帧 |
| v1 | 6 | 1e-3 | 0.83 (1/1.2) | Adam | 稳定在30%左右 | 6%-6.5%之间波动 | 使用前面训练好的模型(下同) |
| v1 | 6 | 1e-4 | 0.83 (1/1.2) | Adam | 训练到32% | 6%-6.5%之间波动 | 经常出现连续一两轮正确率不上升的情况, 猜测衰减速度太慢 |
| v1 | 6 | 1e-4 | 0.5 | Adam | 训练到32% | 6%-6.5%之间波动 | 经常出现连续一两轮正确率不上升的情况, 猜测衰减速度太慢 |
| v1 | 6 | 1e-4 | 0.2 | Adam | 训练到37% | 8% | |
| v1 | 6 | 1e-2 | 1 | Adam | 0.8% | 1.1%,0.5%均有 | 训练集在0.8%上下波动，测试集不稳定，运气好1.1%，运气不好0.5% |
| v1 | 6 | 2e-3 | 0.83（1/1.2） | Adam | - | - | 因训练效果和1e-3时差别不大，训练几轮之后没有在继续 |
| v2（只选取2个样本） | 3 | 1e-3 | 1 | Adam | 100% | - | 训练几次才出现训练到100%的情况，需要运气 |
| v2（只选取2个样本） | 3 | 1e-3 | 1 | SGD | 50% | - | - | |
| v2（只选取2个样本） | 6 | 1e-3 | 1 | Adam | 50% | - | 与上面的结果相比，感觉是运气导致的 |
| v2（只选取2个样本） | 6 | 1e-3 | 1 | SGD | 50% | - | - | |
| v2（只选取2个样本） | 3 | 1e-2 | 1 | Adam | 50% | - | |
| v2（只选取2个样本） | 3 | 1e-2 | 1 | SGD | 50% | - | - | |
| v2（只选取2个样本） | 6 | 1e-2 | 1 | Adam | 50% | - | |
| v2（只选取2个样本） | 6 | 1e-2 | 1 | SGD | 50% | - | - | |

## 实验数据(代码中删去resnet的全连接层，选取最后一帧的预测结果)
| 数据集 | 帧数 | 学习率 | 衰减系数 | 优化方法 | 训练集正确率 | 测试集正确率 | 备注 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| v2 | 3 | 1e-2 | 1 | Adam | 1.6% | 1.6% | 稳定，只能训练到这个结果 |
| v2 | 3 | 1e-3 | 1 | Adam | 53.002% | 18.517% | 过拟合了，这是最好的那一轮的数据，虽然后面的训练集正确率有所提升，但是测试集正确率开始下降 |
| v2 | 6 | 1e-3 | 1 | Adam | 51.42% | 22.01% | 在上面那条实验结果的模型的基础上训练 |
| v2 | 6 | 1e-3 | 0.8 | Adam | - | 25.689% | 在上面那条实验结果的模型的基础上训练, 训练集正确率由于中途蓝屏了, 所以没法收集, 测试集是用每一论训练出来保存的模型重新测的。从这里开始，我把数据保存到文件里面去了 |
| v2 | 10 | 1e-3 | 1 | Adam | - | - |  |